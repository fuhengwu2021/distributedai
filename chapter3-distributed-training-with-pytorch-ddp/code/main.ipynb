{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b366238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91404697",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, size=10000):\n",
    "        \"\"\"创建一个简单的线性数据集 y = 2x + 3 + 噪声\"\"\"\n",
    "        self.x = torch.randn(size, 1)  # 随机生成 x 值\n",
    "        self.y = 2 * self.x + 3 + 0.1 * torch.randn(size, 1)  # 计算对应的 y 值并添加噪声\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "560b52eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # 简单的线性层，输入输出都是 1 维\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aec5bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_gpu_training():\n",
    "    \"\"\"单卡训练函数，作为性能对比基准\"\"\"\n",
    "    # 超参数设置\n",
    "    batch_size = 32\n",
    "    learning_rate = 0.01\n",
    "    epochs = 10\n",
    "    \n",
    "    # 创建数据集和数据加载器\n",
    "    dataset = SimpleDataset(size=10000)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # 初始化模型、损失函数和优化器\n",
    "    model = SimpleModel()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # 记录训练开始时间\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            # 清零梯度\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 反向传播和参数更新\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # 每 100 个批次打印一次信息\n",
    "            if i % 100 == 99:\n",
    "                print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.5f}')\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    # 计算总训练时间\n",
    "    total_time = time.time() - start_time\n",
    "    print(f'单卡训练完成，总时间: {total_time:.2f}秒')\n",
    "    \n",
    "    # 打印学到的参数，应该接近 w=2, b=3\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f'{name}: {param.item():.4f}')\n",
    "    \n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51da1ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] loss: 4.06866\n",
      "[1, 200] loss: 0.07826\n",
      "[1, 300] loss: 0.01117\n",
      "[2, 100] loss: 0.00984\n",
      "[2, 200] loss: 0.01008\n",
      "[2, 300] loss: 0.00992\n",
      "[3, 100] loss: 0.00988\n",
      "[3, 200] loss: 0.00961\n",
      "[3, 300] loss: 0.01024\n",
      "[4, 100] loss: 0.00942\n",
      "[4, 200] loss: 0.01038\n",
      "[4, 300] loss: 0.00975\n",
      "[5, 100] loss: 0.00979\n",
      "[5, 200] loss: 0.00993\n",
      "[5, 300] loss: 0.01007\n",
      "[6, 100] loss: 0.01006\n",
      "[6, 200] loss: 0.00952\n",
      "[6, 300] loss: 0.01008\n",
      "[7, 100] loss: 0.01016\n",
      "[7, 200] loss: 0.00974\n",
      "[7, 300] loss: 0.00994\n",
      "[8, 100] loss: 0.01039\n",
      "[8, 200] loss: 0.00971\n",
      "[8, 300] loss: 0.00969\n",
      "[9, 100] loss: 0.00976\n",
      "[9, 200] loss: 0.00988\n",
      "[9, 300] loss: 0.01008\n",
      "[10, 100] loss: 0.01008\n",
      "[10, 200] loss: 0.00979\n",
      "[10, 300] loss: 0.00986\n",
      "单卡训练完成，总时间: 0.52秒\n",
      "linear.weight: 1.9982\n",
      "linear.bias: 3.0014\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;36m0.5208249092102051\u001b[0m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_gpu_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebe60da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddp_train(rank, world_size, epochs, batch_size, learning_rate):\n",
    "    # 初始化进程组，使用 NCCL 后端（适合 GPU）\n",
    "    # 对于 CPU 训练，可以使用 gloo 后端\n",
    "    \n",
    "    # 支持单GPU模拟：如果使用 torchrun，LOCAL_RANK 会被自动设置\n",
    "    # 否则使用传入的 rank\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', rank))\n",
    "    \n",
    "    # 如果使用 torchrun，它会自动初始化进程组\n",
    "    # 否则手动初始化\n",
    "    if not dist.is_initialized():\n",
    "        dist.init_process_group(\n",
    "            backend='nccl',  # 通信后端\n",
    "            init_method='tcp://127.0.0.1:12355',  # 初始化方法和地址\n",
    "            rank=rank,  # 当前进程编号\n",
    "            world_size=world_size  # 总进程数\n",
    "        )\n",
    "    else:\n",
    "        # torchrun 已经初始化了，使用环境变量中的 rank\n",
    "        rank = int(os.environ.get('RANK', rank))\n",
    "        world_size = int(os.environ.get('WORLD_SIZE', world_size))\n",
    "    \n",
    "    # 设置当前设备 - 使用 local_rank 而不是 rank\n",
    "    # 在单GPU模拟时，所有进程的 local_rank 会映射到同一个GPU\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    \n",
    "    # 创建数据集\n",
    "    dataset = SimpleDataset(size=10000)\n",
    "    \n",
    "    # 创建分布式采样器，确保每个进程获取不同的数据子集\n",
    "    sampler = DistributedSampler(dataset, shuffle=True)\n",
    "    \n",
    "    # 创建数据加载器，注意这里的 batch_size 是每个进程的 batch size\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler  # 使用分布式采样器\n",
    "    )\n",
    "    \n",
    "    # 创建模型并移动到当前设备\n",
    "    model = SimpleModel().to(local_rank)\n",
    "    \n",
    "    # 使用 DDP 包装模型\n",
    "    ddp_model = DDP(model, device_ids=[local_rank])\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # 记录训练开始时间（只在主进程记录）\n",
    "    start_time = time.time() if rank == 0 else None\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(epochs):\n",
    "        # 设置采样器的 epoch，确保不同 epoch 的 shuffle 一致\n",
    "        sampler.set_epoch(epoch)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            # 将数据移动到当前设备\n",
    "            inputs = inputs.to(local_rank)\n",
    "            labels = labels.to(local_rank)\n",
    "            \n",
    "            # 清零梯度\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = ddp_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            \n",
    "            # 参数更新\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # 只在主进程打印信息，避免多个进程同时打印\n",
    "            if rank == 0 and i % 100 == 99:\n",
    "                print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.5f}')\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    # 计算总训练时间\n",
    "    if rank == 0:\n",
    "        total_time = time.time() - start_time\n",
    "        print(f'DDP 训练完成，总时间: {total_time:.2f}秒')\n",
    "        \n",
    "        # 打印学到的参数\n",
    "        for name, param in model.named_parameters():\n",
    "            print(f'{name}: {param.item():.4f}')\n",
    "    \n",
    "    # 清理进程组\n",
    "    dist.destroy_process_group()\n",
    "    \n",
    "    return total_time if rank == 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bea7df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ddp_training(world_size, epochs=10, batch_size=32, learning_rate=0.01):\n",
    "    \"\"\"启动多个进程进行 DDP 训练\"\"\"\n",
    "    # 使用 torch.multiprocessing.spawn 启动多个进程\n",
    "    # 每个进程将运行 ddp_train 函数，并传入不同的 rank\n",
    "    mp.spawn(\n",
    "        ddp_train,  # 要在每个进程中运行的函数\n",
    "        args=(world_size, epochs, batch_size, learning_rate),  # 传递给 ddp_train 的参数\n",
    "        nprocs=world_size,  # 进程数量\n",
    "        join=True  # 是否等待所有进程完成\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b69655",
   "metadata": {},
   "source": [
    "## 单GPU模拟多GPU运行DDP\n",
    "\n",
    "如果你只有一个GPU，但想测试多GPU的DDP训练，可以使用 `torchrun` 在单个GPU上模拟多个进程。\n",
    "\n",
    "### 方法1: 使用 torchrun（推荐）\n",
    "\n",
    "在终端中运行以下命令（从项目根目录）：\n",
    "\n",
    "```bash\n",
    "# 模拟2个GPU（在GPU 0上运行2个进程）\n",
    "CUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node=2 code/chapter3/train_ddp_torchrun.py\n",
    "\n",
    "# 模拟4个GPU（在GPU 0上运行4个进程）\n",
    "CUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node=4 code/chapter3/train_ddp_torchrun.py\n",
    "```\n",
    "\n",
    "### 方法2: 在notebook中使用（需要修改代码）\n",
    "\n",
    "如果你想在notebook中直接运行，需要修改上面的代码，使用环境变量来设置进程数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b56ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子，确保实验可复现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 超参数设置\n",
    "epochs = 10\n",
    "batch_size_per_gpu = 32  # 每个 GPU 的 batch size\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 检查可用 GPU 数量\n",
    "available_gpus = torch.cuda.device_count()\n",
    "print(f\"可用 GPU 数量: {available_gpus}\")\n",
    "\n",
    "# 如果没有可用 GPU，使用 CPU 进行演示（实际中 DDP 通常用于 GPU）\n",
    "if available_gpus == 0:\n",
    "    print(\"警告: 未检测到 GPU，将使用 CPU 进行演示\")\n",
    "    # 单卡(CPU)训练\n",
    "    print(\"\\n===== 开始单卡(CPU)训练 =====\")\n",
    "    single_time = single_gpu_training()\n",
    "    \n",
    "    # 由于没有 GPU，这里不运行 DDP 训练\n",
    "    print(\"\\n 由于没有可用 GPU，跳过 DDP 训练演示\")\n",
    "\n",
    "\n",
    "# 单卡训练\n",
    "#print(\"\\n===== 开始单卡训练 =====\")\n",
    "#single_time = single_gpu_training()\n",
    "\n",
    "# 使用所有可用 GPU 进行 DDP 训练\n",
    "world_size = available_gpus\n",
    "total_batch_size = batch_size_per_gpu * world_size\n",
    "print(f\"\\n===== 开始 DDP 训练 (使用{world_size}个 GPU，总 batch size: {total_batch_size}) =====\")\n",
    "\n",
    "# 为了公平比较，DDP 训练的总 batch size 应与单卡训练相同\n",
    "# 因此每个 GPU 的 batch size = 单卡 batch size / GPU 数量\n",
    "adjusted_batch_size = batch_size_per_gpu\n",
    "\n",
    "# 启动 DDP 训练\n",
    "run_ddp_training(\n",
    "    world_size=world_size,\n",
    "    epochs=epochs,\n",
    "    batch_size=adjusted_batch_size,\n",
    "    learning_rate=learning_rate * world_size  # 当总 batch size 增加时，通常需要按比例增加学习率\n",
    ")\n",
    "\n",
    "# 注意：由于 mp.spawn 的限制，我们无法直接获取 DDP 训练时间\n",
    "# 在实际应用中，可以通过文件或其他方式在进程间传递这个信息\n",
    "print(\"\\n===== 训练对比 =====\")\n",
    "print(f\"单卡训练时间: {single_time:.2f}秒\")\n",
    "print(f\"使用{world_size}个 GPU 的 DDP 训练时间: 请查看上面的 DDP 训练输出\")\n",
    "print(f\"理论加速比: {single_time / (single_time / world_size):.2f}x (实际加速比可能因通信开销略低)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
