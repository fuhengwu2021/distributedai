{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc6dfe3",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction to Modern Distributed AI\n",
    "\n",
    "This notebook contains all code examples from Chapter 1, organized into logical sections for hands-on experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a99a62",
   "metadata": {},
   "source": [
    "## 1. Setup and CUDA Check\n",
    "\n",
    "First, verify your GPU setup and CUDA availability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3f7f667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 8\n",
      "GPU 0: NVIDIA H200\n",
      "GPU 1: NVIDIA H200\n",
      "GPU 2: NVIDIA H200\n",
      "GPU 3: NVIDIA H200\n",
      "GPU 4: NVIDIA H200\n",
      "GPU 5: NVIDIA H200\n",
      "GPU 6: NVIDIA H200\n",
      "GPU 7: NVIDIA H200\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_cuda():\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "check_cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8da8feb",
   "metadata": {},
   "source": [
    "## 2. GPU-Friendly Configuration\n",
    "\n",
    "Example configuration for GPU-friendly training settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d498dd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 32\n",
      "sequence_length: 2048\n",
      "precision: bf16\n",
      "gradient_checkpointing: True\n",
      "gradient_accumulation_steps: 4\n"
     ]
    }
   ],
   "source": [
    "# GPU-friendly configuration example\n",
    "config = {\n",
    "    'batch_size': 32,  # Fits in GPU memory\n",
    "    'sequence_length': 2048,  # Reasonable for most GPUs\n",
    "    'precision': 'bf16',  # Better than FP32, more stable than FP16\n",
    "    'gradient_checkpointing': True,  # Save memory\n",
    "    'gradient_accumulation_steps': 4  # Effective batch size = 128\n",
    "}\n",
    "\n",
    "def print_config():\n",
    "    for k, v in config.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7b5fda",
   "metadata": {},
   "source": [
    "## 3. Single-GPU Baseline Training\n",
    "\n",
    "Single-GPU training baseline for comparison with distributed training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea833dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.3584\n",
      "Epoch 2/10, Loss: 0.9866\n",
      "Epoch 3/10, Loss: 0.2530\n",
      "Epoch 4/10, Loss: 0.0666\n",
      "Epoch 5/10, Loss: 0.0317\n",
      "Epoch 6/10, Loss: 0.0204\n",
      "Epoch 7/10, Loss: 0.0145\n",
      "Epoch 8/10, Loss: 0.0111\n",
      "Epoch 9/10, Loss: 0.0088\n",
      "Epoch 10/10, Loss: 0.0072\n",
      "\n",
      "Total training time: 0.48s\n",
      "Peak memory: 0.07 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_size=1000, hidden_size=512, output_size=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "def train_single_gpu():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SimpleModel().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Create dummy dataset\n",
    "    dataset = TensorDataset(\n",
    "        torch.randn(1000, 1000),\n",
    "        torch.randint(0, 10, (1000,))\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/10, Loss: {epoch_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal training time: {total_time:.2f}s\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Peak memory: {torch.cuda.max_memory_allocated()/1024**3:.2f} GB\")\n",
    "\n",
    "# Uncomment to run:\n",
    "train_single_gpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbd5493",
   "metadata": {},
   "source": [
    "## 4. Distributed Basic Test\n",
    "\n",
    "Basic distributed test to verify process group initialization works. This script tests the fundamental distributed setup: process group initialization, rank identification, and basic communication.\n",
    "\n",
    "**Note:** This requires running with `torchrun` from command line:\n",
    "```bash\n",
    "OMP_NUM_THREADS=8 torchrun --nproc_per_node=2 code/chapter1/ch01_distributed_basic_test.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c770388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "def test_distributed_setup():\n",
    "    \"\"\"Test basic distributed process group initialization and communication\"\"\"\n",
    "    dist.init_process_group(\"nccl\")\n",
    "    rank = dist.get_rank()\n",
    "    print(f\"Rank {rank} says hello.\")\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "# Note: This must be run with torchrun, not directly in notebook\n",
    "# if __name__ == \"__main__\":\n",
    "#     test_distributed_setup()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89cbba3",
   "metadata": {},
   "source": [
    "## 5. Multi-GPU Simulation (Single GPU)\n",
    "\n",
    "Single-GPU simulation of multi-GPU distributed training. This allows you to test distributed training code on a single GPU by simulating multiple processes.\n",
    "\n",
    "**Note:** This requires running with `torchrun` from command line:\n",
    "```bash\n",
    "# Option 1: With MPS (recommended)\n",
    "sudo nvidia-cuda-mps-control -d\n",
    "CUDA_VISIBLE_DEVICES=0 OMP_NUM_THREADS=4 torchrun --nproc_per_node=2 code/chapter1/ch01_multi_gpu_simulation.py\n",
    "\n",
    "# Option 2: Without MPS\n",
    "CUDA_VISIBLE_DEVICES=0 OMP_NUM_THREADS=4 torchrun --nproc_per_node=2 code/chapter1/ch01_multi_gpu_simulation.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa0806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "def simulate_multi_gpu():\n",
    "    \"\"\"Simulate multi-GPU distributed training on a single GPU\"\"\"\n",
    "    dist.init_process_group(\"nccl\")\n",
    "    rank = dist.get_rank()\n",
    "    print(f\"Rank {rank} says hello.\")\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "# Note: This must be run with torchrun, not directly in notebook\n",
    "# if __name__ == \"__main__\":\n",
    "#     simulate_multi_gpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d81a68",
   "metadata": {},
   "source": [
    "## 6. Multi-GPU DDP Training\n",
    "\n",
    "First multi-GPU distributed training using PyTorch DDP. This is a complete distributed training example using DDP with proper setup, DistributedSampler usage, and cleanup.\n",
    "\n",
    "**Note:** This requires running with `torchrun` from command line:\n",
    "```bash\n",
    "OMP_NUM_THREADS=4 torchrun --nproc_per_node=2 code/chapter1/ch01_multi_gpu_ddp.py\n",
    "# Or use the launch script:\n",
    "bash code/chapter1/ch01_launch_torchrun.sh\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0408100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torch.utils.data import TensorDataset\n",
    "import os\n",
    "\n",
    "def setup():\n",
    "    \"\"\"Initialize the process group using torchrun environment variables\"\"\"\n",
    "    # torchrun sets these environment variables automatically\n",
    "    dist.init_process_group(\"nccl\")\n",
    "    rank = dist.get_rank()\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    return rank, dist.get_world_size(), local_rank\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Clean up the process group\"\"\"\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def train_ddp():\n",
    "    \"\"\"Run distributed training using PyTorch DDP\"\"\"\n",
    "    rank, world_size, local_rank = setup()\n",
    "    \n",
    "    # Create model\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(1000, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 10)\n",
    "    ).cuda()\n",
    "    \n",
    "    # Wrap with DDP\n",
    "    model = DDP(model, device_ids=[local_rank])\n",
    "    \n",
    "    # Create dataset with distributed sampler\n",
    "    dataset = TensorDataset(\n",
    "        torch.randn(1000, 1000),\n",
    "        torch.randint(0, 10, (1000,))\n",
    "    )\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(10):\n",
    "        sampler.set_epoch(epoch)  # Important for shuffling\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(f\"Epoch {epoch+1}/10, Loss: {epoch_loss/len(dataloader):.4f}\", flush=True)\n",
    "    \n",
    "    cleanup()\n",
    "\n",
    "# Note: This must be run with torchrun, not directly in notebook\n",
    "# if __name__ == \"__main__\":\n",
    "#     train_ddp()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ea499c",
   "metadata": {},
   "source": [
    "## 7. Profiling and Performance Analysis\n",
    "\n",
    "Memory and latency profiling for model training. This demonstrates how to use PyTorch's profiler to measure CUDA operations and memory usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef549301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "def profile_model():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available, skipping profiling\")\n",
    "        return\n",
    "        \n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(1000, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 10)\n",
    "    ).cuda()\n",
    "    \n",
    "    inputs = torch.randn(32, 1000).cuda()\n",
    "    targets = torch.randint(0, 10, (32,)).cuda()\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    # Memory profiling\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # Time profiling with PyTorch profiler\n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "        with record_function(\"forward\"):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        with record_function(\"backward\"):\n",
    "            loss.backward()\n",
    "        \n",
    "        with record_function(\"optimizer_step\"):\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Print results\n",
    "    print(\"=\" * 80)\n",
    "    print(\"CUDA Time Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"cuda_time_total\",\n",
    "        row_limit=20\n",
    "    ))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Memory Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"cuda_memory_usage\",\n",
    "        row_limit=20\n",
    "    ))\n",
    "    \n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    print(f\"\\nPeak GPU Memory: {peak_memory:.2f} GB\")\n",
    "\n",
    "# Uncomment to run:\n",
    "# profile_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e29b74",
   "metadata": {},
   "source": [
    "## 8. Common DDP Pitfalls\n",
    "\n",
    "Examples of common mistakes and correct patterns in distributed training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d431dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def wrong_master_port(rank):\n",
    "    # Wrong: Each process uses different port\n",
    "    os.environ['MASTER_PORT'] = str(12355 + rank)  # ❌\n",
    "\n",
    "def correct_master_port():\n",
    "    # Correct: All processes use same port\n",
    "    os.environ['MASTER_PORT'] = '12355'  # ✅\n",
    "\n",
    "def wrong_dataloader(dataset):\n",
    "    # Wrong: Each process sees all data\n",
    "    from torch.utils.data import DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=32)  # ❌\n",
    "    return dataloader\n",
    "\n",
    "def correct_dataloader(dataset, world_size, rank):\n",
    "    # Correct: Each process sees subset of data\n",
    "    from torch.utils.data import DataLoader, DistributedSampler\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)  # ✅\n",
    "    return dataloader\n",
    "\n",
    "def set_epoch_for_shuffling(sampler, epoch):\n",
    "    # Correct: Shuffle data each epoch\n",
    "    sampler.set_epoch(epoch)  # ✅\n",
    "\n",
    "print(\"DDP Pitfalls examples defined. See functions above for correct patterns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb8c079",
   "metadata": {},
   "source": [
    "## 9. Measure Training Components\n",
    "\n",
    "Helper function to measure and report the time breakdown of different training components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80ca963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_components(measure_data_loading_time, measure_compute_time, measure_communication_time):\n",
    "    \"\"\"\n",
    "    Measure and report the time breakdown of training components.\n",
    "    \n",
    "    Args:\n",
    "        measure_data_loading_time: Function that returns data loading time\n",
    "        measure_compute_time: Function that returns computation time\n",
    "        measure_communication_time: Function that returns communication time\n",
    "    \"\"\"\n",
    "    data_time = measure_data_loading_time()\n",
    "    compute_time = measure_compute_time()\n",
    "    comm_time = measure_communication_time()\n",
    "\n",
    "    total_time = data_time + compute_time + comm_time\n",
    "    print(f\"Data loading: {data_time/total_time*100:.1f}%\")\n",
    "    print(f\"Computation: {compute_time/total_time*100:.1f}%\")\n",
    "    print(f\"Communication: {comm_time/total_time*100:.1f}%\")\n",
    "\n",
    "# Example usage:\n",
    "# measure_components(\n",
    "#     lambda: 0.2,  # data loading time\n",
    "#     lambda: 0.6,  # computation time\n",
    "#     lambda: 0.2   # communication time\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcab1610",
   "metadata": {},
   "source": [
    "## 10. Running Distributed Training in Notebook\n",
    "\n",
    "While `torchrun` is designed for command-line use, there are ways to run distributed training in Jupyter notebooks. Here are the main approaches:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d12f50",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6e4173",
   "metadata": {},
   "source": [
    "### Option 2: Using subprocess to call torchrun (Not Recommended)\n",
    "\n",
    "You can use subprocess to call torchrun, but this is not ideal as it runs outside the notebook context and output capture can be tricky.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4d5812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def run_torchrun_in_notebook(script_path, nproc_per_node=2):\n",
    "    \"\"\"\n",
    "    Run torchrun via subprocess.\n",
    "    Note: This runs outside the notebook context.\n",
    "    \"\"\"\n",
    "    # Set environment variables\n",
    "    env = os.environ.copy()\n",
    "    env['OMP_NUM_THREADS'] = '4'\n",
    "    \n",
    "    # Build command\n",
    "    cmd = [\n",
    "        sys.executable, '-m', 'torch.distributed.run',\n",
    "        '--nproc_per_node', str(nproc_per_node),\n",
    "        script_path\n",
    "    ]\n",
    "    \n",
    "    # Run and capture output\n",
    "    result = subprocess.run(\n",
    "        cmd,\n",
    "        env=env,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    print(\"STDOUT:\")\n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"\\nSTDERR:\")\n",
    "        print(result.stderr)\n",
    "    \n",
    "    return result.returncode == 0\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# run_torchrun_in_notebook('code/chapter1/ch01_multi_gpu_ddp.py', nproc_per_node=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ff29f",
   "metadata": {},
   "source": [
    "### Option 3: Using IPython Magic Command (Recommended for Notebooks)\n",
    "\n",
    "You can use IPython magic commands to run shell commands, including torchrun.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2ce80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using IPython magic command to run torchrun\n",
    "# This allows you to run torchrun commands directly in the notebook\n",
    "\n",
    "# Example:\n",
    "# !OMP_NUM_THREADS=4 torchrun --nproc_per_node=2 code/chapter1/ch01_multi_gpu_ddp.py\n",
    "\n",
    "# Or with environment variable:\n",
    "# import os\n",
    "# os.environ['OMP_NUM_THREADS'] = '4'\n",
    "# !torchrun --nproc_per_node=2 code/chapter1/ch01_multi_gpu_ddp.py\n",
    "\n",
    "print(\"To run torchrun in notebook, use IPython magic command:\")\n",
    "print(\"!OMP_NUM_THREADS=4 torchrun --nproc_per_node=2 code/chapter1/ch01_multi_gpu_ddp.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b68fb2",
   "metadata": {},
   "source": [
    "### Option 4: Using Hugging Face Accelerate (Alternative Approach)\n",
    "\n",
    "If you have `accelerate` installed, you can use `notebook_launcher` for distributed training in notebooks. This is a cleaner approach for notebook environments.\n",
    "\n",
    "**Note:** This requires installing accelerate: `pip install accelerate`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d70dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using Hugging Face accelerate (if installed)\n",
    "# Uncomment and install accelerate first: pip install accelerate\n",
    "\n",
    "# from accelerate import notebook_launcher\n",
    "# from accelerate import Accelerator\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# def training_function():\n",
    "#     accelerator = Accelerator()\n",
    "#     \n",
    "#     model = nn.Sequential(\n",
    "#         nn.Linear(1000, 512),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Linear(512, 10)\n",
    "#     )\n",
    "#     \n",
    "#     dataset = TensorDataset(\n",
    "#         torch.randn(1000, 1000),\n",
    "#         torch.randint(0, 10, (1000,))\n",
    "#     )\n",
    "#     dataloader = DataLoader(dataset, batch_size=32)\n",
    "#     \n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     \n",
    "#     model, dataloader, optimizer = accelerator.prepare(model, dataloader, optimizer)\n",
    "#     \n",
    "#     model.train()\n",
    "#     for epoch in range(10):\n",
    "#         epoch_loss = 0.0\n",
    "#         for data, target in dataloader:\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(data)\n",
    "#             loss = criterion(output, target)\n",
    "#             accelerator.backward(loss)\n",
    "#             optimizer.step()\n",
    "#             epoch_loss += loss.item()\n",
    "#         \n",
    "#         if accelerator.is_main_process:\n",
    "#             print(f\"Epoch {epoch+1}/10, Loss: {epoch_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# notebook_launcher(training_function, num_processes=2)\n",
    "\n",
    "print(\"Accelerate example code shown above. Install accelerate to use this approach.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c92bae4",
   "metadata": {},
   "source": [
    "### Summary: Running Distributed Training in Notebooks\n",
    "\n",
    "**Best Practices:**\n",
    "1. **For quick command execution**: Use IPython magic `!torchrun ...` (Option 2) - simplest\n",
    "2. **For production-like setup**: Use command line with `torchrun` - most reliable\n",
    "3. **For Hugging Face workflows**: Use `accelerate notebook_launcher` (Option 3) - cleanest for notebooks\n",
    "\n",
    "**Important Notes:**\n",
    "- Notebook environments can have limitations with multiprocessing\n",
    "- Some distributed operations may not work perfectly in all notebook environments\n",
    "- For production training, command-line `torchrun` is still recommended\n",
    "- Make sure you have the required number of GPUs available\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
