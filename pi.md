Title –  Modern Distributed AI Systems
(Example - https://www.amazon.com/dp/1801079315)
Subtitle
Training, Inference, and Serving at Scale

Packtpub Metadescription
Master distributed AI systems with hands-on PyTorch DDP, FSDP, DeepSpeed, vLLM, SGLang, and Kubernetes deployment. Production-ready code for training and serving LLMs at scale.

Key features
(3-4 points, 100 characters per point)
    • Hands-on production code for DDP, FSDP, DeepSpeed, vLLM, and SGLang
    • Complete Kubernetes deployment guide with GPU scheduling and autoscaling
    • Real-world benchmarking and performance optimization techniques
    • End-to-end serving stack with API gateways, tracing, and canary deployments

Approach
This book takes a hands-on, code-first approach. Every chapter includes production-ready, runnable examples you can execute immediately. You'll build distributed training with PyTorch DDP/FSDP/DeepSpeed, deploy inference with vLLM/SGLang, and create production serving stacks on Kubernetes. Focus is on practical implementation and real-world optimization.

Short description
Master distributed AI systems with hands-on PyTorch, DeepSpeed, vLLM, SGLang, and Kubernetes. Build production-ready training and inference pipelines for large language models.

Long description
Modern AI models have grown beyond what single GPUs can handle. Training and serving large language models requires distributed architectures, but most resources focus on theory rather than practical implementation. This book bridges that gap with hands-on, production-ready code for every major distributed AI technology.

You'll start by understanding GPU hardware, networking, and parallelism strategies, then dive deep into distributed training with PyTorch DDP, FSDP, and DeepSpeed ZeRO. The book covers memory optimization, checkpointing, and multi-node scaling with real examples you can run immediately.

The second part focuses on distributed inference and production deployment. You'll build inference systems with vLLM's PagedAttention and SGLang's efficient operator fusion, deploy on Kubernetes with GPU scheduling and autoscaling, and create complete production serving stacks with API gateways, distributed tracing, and canary deployments.

The final section covers benchmarking with genai-bench and MLPerf, performance optimization techniques, and specialized paradigms like federated learning and edge computing. Every chapter includes production-ready code examples tested on real infrastructure, debugging guides, and performance optimization strategies. Whether you're training billion-parameter models or serving them at scale, this book provides the practical knowledge and code you need to build production distributed AI systems.

What will you learn
(7-8 points, 70 characters per point)
    • Implement distributed training with PyTorch DDP, FSDP, and DeepSpeed
    • Build production inference systems with vLLM and SGLang
    • Deploy AI workloads on Kubernetes with GPU scheduling
    • Optimize memory usage and communication in distributed setups
    • Create production serving stacks with API gateways and tracing
    • Benchmark distributed systems with genai-bench and MLPerf
    • Implement federated learning and edge computing architectures
    • Debug and optimize multi-node training and inference pipelines

Audience
This book is for ML engineers, AI researchers, and DevOps engineers who need to train or serve large language models at scale. Readers should have intermediate Python experience and familiarity with PyTorch basics. Prior knowledge of distributed systems or Kubernetes is helpful but not required—the book covers these concepts from the ground up. Whether you're scaling a model that no longer fits on a single GPU, deploying inference systems for production workloads, or optimizing distributed training performance, this book provides the practical knowledge and code you need. The hands-on approach makes it ideal for practitioners who learn by doing.

Author Bio
Henry is an experienced ML engineer and distributed systems practitioner with hands-on expertise in building production AI systems at scale. He has contributed to open-source projects including SGLang and genai-bench, and co-authored technical content with Microsoft and Oracle Cloud on high-performance deep learning. His work focuses on practical implementation of distributed training and inference systems, with a particular emphasis on performance optimization and production deployment. Henry brings real-world experience from working on large-scale distributed AI workloads, making this book grounded in actual production challenges and solutions rather than theoretical concepts alone.

