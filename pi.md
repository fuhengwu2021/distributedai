# Title – Modern Distributed AI Systems

(Example - https://www.amazon.com/dp/1801079315)

## Subtitle

(100 characters)

Build scalable training, inference and serving systems for large AI models using production-ready distributed infrastructure

## Packtpub Metadescription

(230 characters)

Master distributed AI through hands-on experience with training frameworks, inference engines, and orchestration tools to build production-ready training and serving systems for modern large-scale AI.

## Key features

(100 characters per point)

- Understand GPU hardware, high-speed interconnects, and parallelism strategies
- Master distributed training with scalable and resource-optimized techniques
- Deploy high-performance inference with advanced optimization and memory management
- Build production serving stacks with job schedulers, orchestration, and observability

## Approach

(400 characters)

Complete with hands-on code examples, this practical book guides you to build distributed AI systems from the ground up. You'll learn through working examples that you can run immediately on your own infrastructure, covering everything from distributed training and inference to production serving.

## Short description

(250 characters)

Modern Distributed AI Systems will help you build scalable training and inference systems for large AI models. From hardware and resource estimation to cluster orchestration and deployment, this book shows you how to implement production-ready distributed AI systems.

## Long description

(1350 characters)

With large AI models now reaching billions or even trillions of parameters, distributed systems have become essential for training and serving modern AI. While many resources cover individual aspects, no single book addresses the complete journey from distributed training through inference to production serving. This book bridges that gap with hands-on code examples demonstrating production-grade techniques used in real-world systems.

The book begins with resource estimation (covering GPU and memory requirements) and data preparation, then dives into GPU hardware architecture, high-speed interconnect technologies, and parallelism strategies. You'll learn distributed training techniques including data parallelism for single and multi-node setups, parameter sharding for memory-efficient training, and memory optimization strategies for massive models.

The second part focuses on distributed inference and production deployment. You'll build high-performance inference systems using advanced attention mechanisms, cache management, operator fusion techniques, and router-based architectures. You'll deploy on cluster schedulers and container orchestration platforms with GPU scheduling, and create complete production serving stacks with essential components for reliability, scalability, and observability.

The final section covers rigorous benchmarking methodologies, performance optimization techniques, and emerging trends including MoE architectures, edge-cloud coordination, and advanced parallelism strategies. Every chapter includes working code examples tested on real infrastructure, debugging guides, and performance optimization strategies.

By the end of the book, you'll be equipped to build production distributed AI systems that scale from single GPU to large clusters, whether you're training large AI models or serving them to millions of users.

## What will you learn

(70 characters per point)

- Estimate memory and compute requirements for training and inference

- Understand GPU hardware, interconnects, and parallelism strategies

- Implement distributed training with parallel and sharded techniques

- Build production inference systems with batching and memory management

- Deploy on cluster schedulers and orchestration platforms with GPU scheduling

- Create production serving stacks with routing and observability

- Benchmark distributed systems using industry-standard methodologies

- Explore emerging trends including MoE architectures and future directions

## Audience

(600 characters)

This book is for ML engineers, AI researchers, and DevOps engineers who need to train or serve large AI models at scale. Platform engineers, HPC cluster administrators, and cloud architects will be able to advance their skill set with this guide. A basic understanding of Python and PyTorch fundamentals is necessary to get started. Prior knowledge of distributed systems, cluster schedulers, or container orchestration is helpful but not required—the book covers these concepts from the ground up, starting with resource estimation, data preparation, and hardware fundamentals.

## Author Bio

(750 characters)

Henry (Fuheng) Wu is a Principal Machine Learning Tech Lead at Oracle’s Generative AI organization, specializing in distributed training, large-scale inference, and GPU systems. He has delivered core components of Oracle’s Vision and Document Understanding AI services, and co-authored a Microsoft + Oracle blog on high-performance deep learning with ONNX Runtime. Henry has contributed to open-source projects including SGLang, genai-bench, pyLLaMA, chatLLaMA, and Oracle’s HiQ observability system. With hands-on experience across PyTorch Distributed, DeepSpeed, Kubernetes GPU clusters, and production LLM serving, he focuses on building practical, scalable AI systems used in real-world enterprise workloads.

